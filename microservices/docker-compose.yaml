services:
  backend_1:
    build: 
      context: backend
      dockerfile: Dockerfile
    user: "0:0"
    env_file:
      - ./backend/.env
    depends_on:
      haproxy_db_1:
        condition: service_healthy
      haproxy_db_2:
        condition: service_healthy
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 60s
    dns:
      - 172.31.0.53
    networks:
      - lb-net

  backend_2:
    build: 
      context: backend
      dockerfile: Dockerfile
    user: "0:0"
    env_file:
      - ./backend/.env
    depends_on:
      haproxy_db_1:
        condition: service_healthy
      haproxy_db_2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 60s
    dns:
      - 172.31.0.53
    networks:
      - lb-net
    
  frontend_1:
    build:
      context: frontend
      dockerfile: Dockerfile
    user: "0:0"
    env_file:
      - ./frontend/.env
    depends_on:
      backend_1:
        condition: service_healthy
      backend_2:
        condition: service_healthy
    dns:
      - 172.31.0.53
    networks:
      - lb-net
    
  frontend_2:
    build:
      context: frontend
      dockerfile: Dockerfile
    user: "0:0"
    env_file:
      - ./frontend/.env
    depends_on:
      backend_1:
        condition: service_healthy
      backend_2:
        condition: service_healthy
    dns:
      - 172.31.0.53
    networks:
      - lb-net
  coredns:
    image: coredns/coredns:1.11.3
    command: -conf /etc/coredns/Corefile
    user: "0:0"
    volumes:
      - ./coredns/Corefile:/etc/coredns/Corefile:ro
      - ./coredns/db.app.local:/zones/db.app.local:ro
    ports:
      - "1053:53/udp"
      - "1053:53/tcp"
    networks:
      lb-net:
        ipv4_address: 172.31.0.53

  haproxy_1:
    image: haproxy:2.9
    restart: always
    user: "0:0"
    ports:
      - "80:80"
    volumes:
      - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      backend_1:
        condition: service_healthy
      backend_2:
        condition: service_healthy
    networks:
      lb-net:
        ipv4_address: 172.31.0.100

  haproxy_2:
    image: haproxy:2.9
    restart: always
    user: "0:0"
    ports:
      - "81:80"
    volumes:
      - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      backend_1:
        condition: service_healthy
      backend_2:
        condition: service_healthy
    networks:
      lb-net:
        ipv4_address: 172.31.0.101
  etcd:
    image: quay.io/coreos/etcd:v3.5.16
    restart: always
    user: "0:0"
    environment:
      - ETCDCTL_API=3
    command: >
      etcd
      --name etcd0
      --data-dir /etcd-data
      --advertise-client-urls http://etcd:2379
      --listen-client-urls http://0.0.0.0:2379
      --listen-peer-urls http://0.0.0.0:2380
      --initial-advertise-peer-urls http://etcd:2380
      --initial-cluster etcd0=http://etcd:2380
      --initial-cluster-state new
      --initial-cluster-token patroni-etcd-cluster
      --enable-v2=true
    volumes:
      - ./data/etcd_data:/etcd-data
    healthcheck:
      test: ["CMD", "etcdctl", "--endpoints=http://127.0.0.1:2379", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    networks:
      - lb-net

  patroni_1:
    image: ghcr.io/zalando/spilo-16:3.3-p1
    restart: always
    user: "0:0"
    environment:
      - SCOPE=pg-cluster
      - ETCD_HOST=etcd:2379
      - PGUSER_SUPERUSER=postgres
      - PGPASSWORD_SUPERUSER=postgres_password
      - PGUSER_STANDBY=standby
      - PGPASSWORD_STANDBY=standby_password
      - PGUSER_ADMIN=admin
      - PGPASSWORD_ADMIN=admin_password
      - PGDATA=/home/postgres/pgdata/pgroot/data
      - ALLOW_NOSSL=true
    depends_on:
      etcd:
        condition: service_healthy
    volumes:
      - ./data/patroni_1_data:/home/postgres/pgdata
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8008/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - lb-net

  patroni_2:
    image: ghcr.io/zalando/spilo-16:3.3-p1
    restart: always
    user: "0:0"
    environment:
      - SCOPE=pg-cluster
      - ETCD_HOST=etcd:2379
      - PGUSER_SUPERUSER=postgres
      - PGPASSWORD_SUPERUSER=postgres_password
      - PGUSER_STANDBY=standby
      - PGPASSWORD_STANDBY=standby_password
      - PGUSER_ADMIN=admin
      - PGPASSWORD_ADMIN=admin_password
      - PGDATA=/home/postgres/pgdata/pgroot/data
      - ALLOW_NOSSL=true
    depends_on:
      etcd:
        condition: service_healthy
    volumes:
      - ./data/patroni_2_data:/home/postgres/pgdata
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8008/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - lb-net

  pgpool_1:
    build:
      context: pgpool
      dockerfile: Dockerfile
    restart: always
    user: "0:0"
    depends_on:
      patroni_1:
        condition: service_healthy
      patroni_2:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres -h localhost -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - lb-net

  pgpool_2:
    build:
      context: pgpool
      dockerfile: Dockerfile
    restart: always
    user: "0:0"
    depends_on:
      patroni_1:
        condition: service_healthy
      patroni_2:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres -h localhost -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - lb-net

  haproxy_db_1:
    image: haproxy:2.9
    restart: always
    user: "0:0"
    volumes:
      - ./haproxy/haproxy-db.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      pgpool_1:
        condition: service_healthy
      pgpool_2:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      lb-net:
        ipv4_address: 172.31.0.102

  haproxy_db_2:
    image: haproxy:2.9
    restart: always
    user: "0:0"
    volumes:
      - ./haproxy/haproxy-db.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      pgpool_1:
        condition: service_healthy
      pgpool_2:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      lb-net:
        ipv4_address: 172.31.0.103

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    restart: always
    user: "0:0"
    ports:
      - "8088:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - lb-net

  prometheus:
    image: prom/prometheus:v3.2.1
    restart: always
    user: "0:0"
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.external-url=/prometheus/
      - --web.route-prefix=/prometheus/
    ports:
      - "9090:9090"
    volumes:
      - ./observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./data/prometheus:/prometheus
    depends_on:
      cadvisor:
        condition: service_started
      blackbox_exporter:
        condition: service_started
      backend_1:
        condition: service_healthy
      backend_2:
        condition: service_healthy
    networks:
      - lb-net

  loki:
    image: grafana/loki:3.4.1
    restart: always
    user: "0:0"
    command: -config.file=/etc/loki/config.yml
    ports:
      - "3100:3100"
    volumes:
      - ./observability/loki/loki-config.yml:/etc/loki/config.yml:ro
      - ./data/loki:/loki
    networks:
      - lb-net

  blackbox_exporter:
    image: prom/blackbox-exporter:v0.25.0
    restart: always
    user: "0:0"
    command:
      - --config.file=/etc/blackbox_exporter/config.yml
    ports:
      - "9115:9115"
    volumes:
      - ./observability/blackbox/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    networks:
      - lb-net

  vector:
    image: timberio/vector:0.42.0-alpine
    restart: always
    user: "0:0"
    command: ["--config", "/etc/vector/vector.toml"]
    volumes:
      - ./observability/vector/vector.toml:/etc/vector/vector.toml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./data/vector:/var/lib/vector
    depends_on:
      loki:
        condition: service_started
    networks:
      - lb-net

  grafana:
    image: grafana/grafana:11.5.2
    restart: always
    user: "0:0"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    ports:
      - "3000:3000"
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./observability/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      prometheus:
        condition: service_started
      loki:
        condition: service_started
    networks:
      - lb-net

networks:
  lb-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/24
